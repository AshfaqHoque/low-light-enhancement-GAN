{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9626961,"sourceType":"datasetVersion","datasetId":5876497},{"sourceId":10642328,"sourceType":"datasetVersion","datasetId":6589369},{"sourceId":248998,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":212103,"modelId":233777}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\nimport kagglehub\nloli_path = kagglehub.dataset_download('tanvirnwu/loli-street-low-light-image-enhancement-of-street')\n\nprint('Data source import complete.')\n","metadata":{"id":"2_DUcvJgC36P","outputId":"a8ac412d-0a38-478f-dec3-e952b9a841b3","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:41:44.534159Z","iopub.execute_input":"2025-02-03T15:41:44.534488Z","iopub.status.idle":"2025-02-03T15:41:44.660515Z","shell.execute_reply.started":"2025-02-03T15:41:44.534462Z","shell.execute_reply":"2025-02-03T15:41:44.659619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os","metadata":{"id":"6FUtCGhdE3--","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:41:44.661624Z","iopub.execute_input":"2025-02-03T15:41:44.661860Z","iopub.status.idle":"2025-02-03T15:41:44.665484Z","shell.execute_reply.started":"2025-02-03T15:41:44.661839Z","shell.execute_reply":"2025-02-03T15:41:44.664673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.listdir(loli_path)","metadata":{"id":"DAbEbhgREZn8","outputId":"0794659d-8ce6-4043-a37c-03646aa0279b","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:41:44.666940Z","iopub.execute_input":"2025-02-03T15:41:44.667179Z","iopub.status.idle":"2025-02-03T15:41:44.684920Z","shell.execute_reply.started":"2025-02-03T15:41:44.667157Z","shell.execute_reply":"2025-02-03T15:41:44.684236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loli_path = os.path.join(loli_path, \"LoLI-Street Dataset\")","metadata":{"id":"MxWkJR3UE7aT","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:41:44.686293Z","iopub.execute_input":"2025-02-03T15:41:44.686583Z","iopub.status.idle":"2025-02-03T15:41:44.699563Z","shell.execute_reply.started":"2025-02-03T15:41:44.686554Z","shell.execute_reply":"2025-02-03T15:41:44.698723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom torchvision import transforms\n\nclass LowLightDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        \"\"\"\n        Args:\n            root_dir (str): Path to the root directory containing 'low' and 'high' folders.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n        self.root_dir = root_dir\n        self.transform = transform\n\n        # Get the list of low and high image filenames\n        self.low_dir = os.path.join(root_dir, 'low')\n        self.high_dir = os.path.join(root_dir, 'high')\n\n        # Ensure the filenames in 'low' and 'high' directories match\n        self.low_images = sorted(os.listdir(self.low_dir))\n        self.high_images = sorted(os.listdir(self.high_dir))\n\n        # Verify that the filenames match\n        assert self.low_images == self.high_images, \"Low and high image filenames do not match!\"\n\n    def __len__(self):\n        return len(self.low_images)\n\n    def __getitem__(self, idx):\n        # Load low and high images\n        low_image_path = os.path.join(self.low_dir, self.low_images[idx])\n        high_image_path = os.path.join(self.high_dir, self.high_images[idx])\n\n        low_image = Image.open(low_image_path).convert('RGB')\n        high_image = Image.open(high_image_path).convert('RGB')\n\n        # Apply transformations if any\n        if self.transform:\n            low_image = self.transform(low_image)\n            high_image = self.transform(high_image)\n\n        return low_image, high_image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"RzcNrfINC36Q","execution":{"iopub.status.busy":"2025-02-03T15:41:44.700473Z","iopub.execute_input":"2025-02-03T15:41:44.700741Z","iopub.status.idle":"2025-02-03T15:41:44.713719Z","shell.execute_reply.started":"2025-02-03T15:41:44.700720Z","shell.execute_reply":"2025-02-03T15:41:44.712942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define transformations\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),  # Resize images to a fixed size\n    transforms.ToTensor(),          # Convert images to PyTorch tensors\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n])","metadata":{"id":"6hKdNAxEEHVE","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:41:44.714424Z","iopub.execute_input":"2025-02-03T15:41:44.714614Z","iopub.status.idle":"2025-02-03T15:41:44.734630Z","shell.execute_reply.started":"2025-02-03T15:41:44.714598Z","shell.execute_reply":"2025-02-03T15:41:44.734062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Define paths\ntrain_path = os.path.join(loli_path, 'Train')\ntest_path = os.path.join(loli_path, 'Test')\nval_path = os.path.join(loli_path, 'Val')\n\n# Create datasets\ntrain_dataset = LowLightDataset(root_dir=train_path, transform=transform)\nval_dataset = LowLightDataset(root_dir=val_path, transform=transform)\n\ntrain_dataset = torch.utils.data.Subset(train_dataset, range(5000))\nval_dataset = torch.utils.data.Subset(val_dataset, range(5000))","metadata":{"id":"GLvLNa2HEJgs","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:41:44.735427Z","iopub.execute_input":"2025-02-03T15:41:44.735697Z","iopub.status.idle":"2025-02-03T15:41:44.792531Z","shell.execute_reply.started":"2025-02-03T15:41:44.735677Z","shell.execute_reply":"2025-02-03T15:41:44.791626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Define batch size\nbatch_size = 32\n\n# Create dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)","metadata":{"id":"2gmNo4fFENNz","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:41:44.794961Z","iopub.execute_input":"2025-02-03T15:41:44.795211Z","iopub.status.idle":"2025-02-03T15:41:44.805115Z","shell.execute_reply.started":"2025-02-03T15:41:44.795190Z","shell.execute_reply":"2025-02-03T15:41:44.804094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torchvision.utils as vutils\n\n# Get a batch of data\nlow_images, high_images = next(iter(train_dataloader))\n\n# Create a figure with two subplots\nfig, axes = plt.subplots(1, 2, figsize=(16, 16))\n\n# Display Low-Light Images\naxes[0].set_title(\"Low-Light Images\")\naxes[0].imshow(vutils.make_grid(low_images, nrow=8, normalize=True).permute(1, 2, 0))\naxes[0].axis(\"off\")\n\n# Display High-Light Images\naxes[1].set_title(\"High-Light Images\")\naxes[1].imshow(vutils.make_grid(high_images, nrow=8, normalize=True).permute(1, 2, 0))\naxes[1].axis(\"off\")\n\nplt.show()\n","metadata":{"id":"CrDZdDkFERLc","outputId":"10175da1-6d4d-4390-f9d0-8a54c9ba1b37","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:41:44.806435Z","iopub.execute_input":"2025-02-03T15:41:44.806712Z","iopub.status.idle":"2025-02-03T15:41:46.271850Z","shell.execute_reply.started":"2025-02-03T15:41:44.806683Z","shell.execute_reply":"2025-02-03T15:41:46.270759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass UNetGenerator(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, num_residual=9):  # Increased num_residual\n        super(UNetGenerator, self).__init__()\n\n        # Initial Convolution\n        self.initial = nn.Sequential(\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(in_channels, 64, kernel_size=7, padding=0),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(True)\n        )\n\n        # Enhanced Downsampling with more channels\n        self.down1 = DownBlock(64, 128)    # /2\n        self.down2 = DownBlock(128, 256)   # /4\n        self.down3 = DownBlock(256, 512)   # /8\n        self.down4 = DownBlock(512, 512)\n\n        # Bottleneck with more Efficient Residual Blocks\n        self.bottleneck = nn.Sequential(*[\n            EfficientResidualBlock(512) for _ in range(num_residual)\n        ])\n\n        # Enhanced Upsampling with Attention-Guided Skip Connections\n        self.up1 = UpBlock(512, 512, 512)\n        self.up2 = UpBlock(512, 256, 512)    # /8\n        self.up3 = UpBlock(256, 128, 256)    # /4\n        self.up4 = UpBlock(128, 64, 128)     # /2\n\n        # Output Layer\n        self.output = nn.Sequential(\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(64, out_channels, kernel_size=7, padding=0),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        # Initial convolution\n        x = self.initial(x)\n\n        # Downsampling with skip connections\n        d1 = self.down1(x)   # 128\n        d2 = self.down2(d1)  # 256\n        d3 = self.down3(d2)  # 512\n        d4 = self.down4(d3)\n\n        # Bottleneck processing\n        x = self.bottleneck(d4)\n\n        # Upsampling with attention-guided skip connections\n        x = self.up1(x, d4)   # 512\n        x = self.up2(x, d3)   # 256\n        x = self.up3(x, d2)   # 128\n        x = self.up4(x, d1)   # 64\n\n        return self.output(x)\n\nclass DownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DownBlock, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n        self.attention = EnhancedChannelSpatialAttention(out_channels)  # Enhanced attention\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.attention(x)\n        return x\n\nclass UpBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, skip_channels):\n        super(UpBlock, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='nearest'),  # Lightweight upsampling\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n        self.attention = EnhancedChannelSpatialAttention(out_channels + skip_channels)  # Enhanced attention\n        self.conv = nn.Sequential(\n            nn.Conv2d(out_channels + skip_channels, out_channels, kernel_size=3, padding=1),\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x, skip):\n        x = self.up(x)\n        skip_resized = F.interpolate(skip, size=x.shape[2:], mode='bilinear', align_corners=False)\n        x = torch.cat([x, skip_resized], dim=1)\n        x = self.attention(x)  # Attention-guided feature fusion\n        return self.conv(x)\n\nclass EfficientResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super(EfficientResidualBlock, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(channels, channels, kernel_size=3, padding=1, groups=channels),\n            nn.Conv2d(channels, channels, kernel_size=1),\n            nn.InstanceNorm2d(channels),\n            nn.ReLU(True),\n            nn.Conv2d(channels, channels, kernel_size=3, padding=1, groups=channels),\n            nn.Conv2d(channels, channels, kernel_size=1),\n            nn.InstanceNorm2d(channels)\n        )\n        self.attention = EnhancedChannelSpatialAttentionWithSE(channels)  # Use combined attention\n\n    def forward(self, x):\n        residual = x\n        x = self.conv(x)\n        x = self.attention(x)  # Apply combined attention\n        return x + residual\n\nclass EnhancedChannelAttention(nn.Module):\n    def __init__(self, channels, reduction=16):  # Increased reduction ratio\n        super(EnhancedChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.fc = nn.Sequential(\n            nn.Linear(channels, channels // reduction),\n            nn.ReLU(True),\n            nn.Linear(channels // reduction, channels)\n        )\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        avg = self.avg_pool(x).view(b, c)\n        max = self.max_pool(x).view(b, c)\n\n        avg_out = self.fc(avg).view(b, c, 1, 1)\n        max_out = self.fc(max).view(b, c, 1, 1)\n\n        out = avg_out + max_out\n        return x * self.sigmoid(out)\n\nclass EnhancedSpatialAttention(nn.Module):\n    def __init__(self):\n        super(EnhancedSpatialAttention, self).__init__()\n        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg = torch.mean(x, dim=1, keepdim=True)\n        max, _ = torch.max(x, dim=1, keepdim=True)\n        combined = torch.cat([avg, max], dim=1)\n        att = self.conv(combined)\n        return x * self.sigmoid(att)\n        \n\nclass EnhancedChannelSpatialAttention(nn.Module):\n    def __init__(self, channels, reduction=16):  # Increased reduction ratio\n        super(EnhancedChannelSpatialAttention, self).__init__()\n        self.channel_att = EnhancedChannelAttention(channels, reduction)\n        self.spatial_att = EnhancedSpatialAttention()\n\n    def forward(self, x):\n        x = self.channel_att(x)\n        x = self.spatial_att(x)\n        return x\n\nclass SEBlock(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channels, channels // reduction),\n            nn.ReLU(inplace=True),\n            nn.Linear(channels // reduction, channels),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n\nclass EnhancedChannelSpatialAttentionWithSE(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super(EnhancedChannelSpatialAttentionWithSE, self).__init__()\n        self.channel_spatial_att = EnhancedChannelSpatialAttention(channels, reduction)\n        self.se_att = SEBlock(channels, reduction)  # Add SE block\n        self.combine = nn.Conv2d(channels * 2, channels, kernel_size=1)  # Combine features\n\n    def forward(self, x):\n        att1 = self.channel_spatial_att(x)  # Apply channel-spatial attention\n        att2 = self.se_att(x)  # Apply SE attention\n        combined = torch.cat([att1, att2], dim=1)  # Concatenate outputs\n        return self.combine(combined)  # Combine features","metadata":{"id":"CUb9q9LEFFtp","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:41:46.272709Z","iopub.execute_input":"2025-02-03T15:41:46.273166Z","iopub.status.idle":"2025-02-03T15:41:46.297732Z","shell.execute_reply.started":"2025-02-03T15:41:46.273138Z","shell.execute_reply":"2025-02-03T15:41:46.296984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PatchGANDiscriminator(nn.Module):\n    def __init__(self, in_channels=3):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(in_channels, 64, 4, stride=2, padding=1)),\n            nn.LeakyReLU(0.2),\n\n            nn.utils.spectral_norm(nn.Conv2d(64, 128, 4, stride=2, padding=1)),\n            nn.InstanceNorm2d(128),\n            nn.LeakyReLU(0.2),\n\n            nn.utils.spectral_norm(nn.Conv2d(128, 256, 4, stride=2, padding=1)),\n            nn.InstanceNorm2d(256),\n            nn.LeakyReLU(0.2),\n\n            nn.utils.spectral_norm(nn.Conv2d(256, 512, 4, stride=1, padding=1)),\n            nn.InstanceNorm2d(512),\n            nn.LeakyReLU(0.2),\n\n            nn.utils.spectral_norm(nn.Conv2d(512, 1, 4, stride=1, padding=1)),\n            nn.Dropout2d(0.3)\n        )\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"id":"YnGxDkjdFrbx","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:41:46.298577Z","iopub.execute_input":"2025-02-03T15:41:46.298890Z","iopub.status.idle":"2025-02-03T15:41:46.321546Z","shell.execute_reply.started":"2025-02-03T15:41:46.298860Z","shell.execute_reply":"2025-02-03T15:41:46.320850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pytorch_msssim","metadata":{"id":"FPdocWx_Fl5T","outputId":"1da49e9b-48ca-4b3c-87ba-c49f3de1de6b","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:41:46.322271Z","iopub.execute_input":"2025-02-03T15:41:46.322484Z","iopub.status.idle":"2025-02-03T15:41:49.715260Z","shell.execute_reply.started":"2025-02-03T15:41:46.322465Z","shell.execute_reply":"2025-02-03T15:41:49.714116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# import torchvision.models as models\n# from pytorch_msssim import ssim\n\n# class PerceptualLoss(nn.Module):\n#     def __init__(self):\n#         super(PerceptualLoss, self).__init__()\n#         self.vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features[:16].eval()\n#         for param in self.vgg.parameters():\n#             param.requires_grad = False\n#         self.criterion = nn.L1Loss()a\n#         self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n#         self.std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n\n#     def forward(self, input, target):\n#         # Ensure input and target have 3 channels\n#         if input.shape[1] != 3:\n#             input = input[:, :3, :, :]\n#         if target.shape[1] != 3:\n#             target = target[:, :3, :, :]\n\n#         # Normalize input and target for VGG\n#         input_vgg = (input - self.mean.to(input.device)) / self.std.to(input.device)\n#         target_vgg = (target - self.mean.to(target.device)) / self.std.to(target.device)\n\n#         # Extract features and compute loss\n#         input_features = self.vgg(input_vgg)\n#         target_features = self.vgg(target_vgg)\n#         return self.criterion(input_features, target_features)\n\n\n# # SSIM Loss\n# class SSIMLoss(nn.Module):\n#     def forward(self, input, target):\n#         # Ensure input and target have 3 channels\n#         if input.shape[1] != 3:\n#             input = input[:, :3, :, :]\n#         if target.shape[1] != 3:\n#             target = target[:, :3, :, :]\n\n#         return 1 - ssim(input, target, data_range=1.0, size_average=True)\n\n\n# # Edge-Preserving Loss (Sobel-based)\n# class EdgePreservingLoss(nn.Module):\n#     def __init__(self):\n#         super(EdgePreservingLoss, self).__init__()\n#         sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n#         sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n#         self.register_buffer(\"sobel_x\", sobel_x)\n#         self.register_buffer(\"sobel_y\", sobel_y)\n\n#     def forward(self, input, target):\n#         # Ensure input and target have the same number of channels\n#         if input.shape[1] != target.shape[1]:\n#             raise ValueError(\"Input and target must have the same number of channels\")\n\n#         # Compute edge maps\n#         input_edges_x = F.conv2d(input, self.sobel_x.repeat(input.shape[1], 1, 1, 1), groups=input.shape[1], padding=1)\n#         input_edges_y = F.conv2d(input, self.sobel_y.repeat(input.shape[1], 1, 1, 1), groups=input.shape[1], padding=1)\n#         input_edges = torch.sqrt(input_edges_x**2 + input_edges_y**2)\n\n#         target_edges_x = F.conv2d(target, self.sobel_x.repeat(target.shape[1], 1, 1, 1), groups=target.shape[1], padding=1)\n#         target_edges_y = F.conv2d(target, self.sobel_y.repeat(target.shape[1], 1, 1, 1), groups=target.shape[1], padding=1)\n#         target_edges = torch.sqrt(target_edges_x**2 + target_edges_y**2)\n\n#         return F.l1_loss(input_edges, target_edges)\n","metadata":{"id":"Z4qRHCI1FbN1","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:41:49.716737Z","iopub.execute_input":"2025-02-03T15:41:49.717072Z","iopub.status.idle":"2025-02-03T15:41:49.721525Z","shell.execute_reply.started":"2025-02-03T15:41:49.717047Z","shell.execute_reply":"2025-02-03T15:41:49.720501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.cuda.amp import GradScaler, autocast\nfrom tqdm import tqdm\n\n# Initialize TensorBoard\nwriter = SummaryWriter(log_dir=\"/kaggle/working/runs/experiment_name\")\n\n# Function to Load Checkpoints with Partial Matching\ndef load_checkpoint(path, generator, discriminator, device):\n    checkpoint = torch.load(path, map_location=device)\n\n    # Load model states with partial matching\n    generator_state_dict = checkpoint['generator']\n    discriminator_state_dict = checkpoint['discriminator']\n    \n    generator_dict = generator.state_dict()\n    discriminator_dict = discriminator.state_dict()\n\n    # Load only matching weights\n    generator_state_dict = {k: v for k, v in generator_state_dict.items() if k in generator_dict}\n    discriminator_state_dict = {k: v for k, v in discriminator_state_dict.items() if k in discriminator_dict}\n\n    generator_dict.update(generator_state_dict)\n    discriminator_dict.update(discriminator_state_dict)\n\n    generator.load_state_dict(generator_dict, strict=False)\n    discriminator.load_state_dict(discriminator_dict, strict=False)\n\n    print(f\"Checkpoint {path} loaded successfully with partial matching!\")\n\n    return checkpoint['epoch']  # Return last completed epoch\n\n\n# Training Loop with Full Checkpoint Support\ndef train(generator, discriminator, dataloader, optimizer_G, optimizer_D, device, \n          start_epoch=0, epochs=100, checkpoint_dir=\"/kaggle/working/checkpoints\"):\n    \n    os.makedirs(checkpoint_dir, exist_ok=True)\n    \n    criterion_GAN = nn.BCEWithLogitsLoss().to(device)\n    scaler = GradScaler()\n    accumulation_steps = 4\n    lambda_L1 = 100\n\n    # Wrap with DataParallel if multiple GPUs are available\n    if torch.cuda.device_count() > 1:\n        generator = nn.DataParallel(generator)\n        discriminator = nn.DataParallel(discriminator)\n\n    for epoch in range(start_epoch, start_epoch + epochs):\n        total_loss_D, total_loss_G, total_loss_GAN, total_loss_L1 = 0, 0, 0, 0\n        current_epoch_in_session = epoch - start_epoch + 1\n        \n        with tqdm(dataloader, desc=f\"Session Epoch {current_epoch_in_session}/{epochs} (Global {epoch+1})\",\n                  unit=\"batch\") as pbar:\n            for i, (low_imgs, high_imgs) in enumerate(pbar):\n                low_imgs, high_imgs = low_imgs.to(device), high_imgs.to(device)\n\n                # Train Discriminator\n                with autocast():\n                    fake_imgs = generator(low_imgs).detach()\n                    real_loss = criterion_GAN(discriminator(high_imgs), torch.ones_like(discriminator(high_imgs)))\n                    fake_loss = criterion_GAN(discriminator(fake_imgs), torch.zeros_like(discriminator(fake_imgs)))\n                    loss_D = (real_loss + fake_loss) / 2\n                scaler.scale(loss_D).backward()\n                if (i + 1) % accumulation_steps == 0:\n                    scaler.step(optimizer_D)\n                    scaler.update()\n                    optimizer_D.zero_grad()\n\n                # Train Generator\n                with autocast():\n                    fake_imgs = generator(low_imgs)\n                    loss_GAN = criterion_GAN(discriminator(fake_imgs), torch.ones_like(discriminator(fake_imgs)))\n                    loss_L1 = F.l1_loss(fake_imgs, high_imgs)\n                    loss_G = loss_GAN + lambda_L1 * loss_L1\n                scaler.scale(loss_G).backward()\n                if (i + 1) % accumulation_steps == 0:\n                    scaler.step(optimizer_G)\n                    scaler.update()\n                    optimizer_G.zero_grad()\n\n                # Track losses\n                total_loss_D += loss_D.item()\n                total_loss_G += loss_G.item()\n                total_loss_GAN += loss_GAN.item()\n                total_loss_L1 += loss_L1.item()\n\n                # Update tqdm display\n                pbar.set_postfix({\n                    \"Loss D\": f\"{loss_D.item():.4f}\",\n                    \"Loss G\": f\"{loss_G.item():.4f}\",\n                    \"GAN\": f\"{loss_GAN.item():.4f}\",\n                    \"L1\": f\"{loss_L1.item():.4f}\"\n                })\n\n                # Free memory\n                del fake_imgs, loss_D, loss_G, loss_GAN, loss_L1\n\n            # Log losses after each epoch\n            writer.add_scalar(\"Loss/Discriminator\", total_loss_D / len(dataloader), epoch)\n            writer.add_scalar(\"Loss/Generator\", total_loss_G / len(dataloader), epoch)\n            writer.add_scalar(\"Loss/GAN\", total_loss_GAN / len(dataloader), epoch)\n            writer.add_scalar(\"Loss/L1\", total_loss_L1 / len(dataloader), epoch)\n\n            print(f\"Session Epoch [{current_epoch_in_session}/{epochs}] (Global {epoch+1}) \"\n                  f\"Loss D: {total_loss_D / len(dataloader):.4f}, \"\n                  f\"Loss G: {total_loss_G / len(dataloader):.4f}, \"\n                  f\"GAN: {total_loss_GAN / len(dataloader):.4f}, \"\n                  f\"L1: {total_loss_L1 / len(dataloader):.4f}\")\n\n        # Save full checkpoint every 5 epochs\n        if (epoch + 1) % 5 == 0:\n            checkpoint = {\n                'epoch': epoch + 1,\n                'generator': generator.module.state_dict() if isinstance(generator, nn.DataParallel) else generator.state_dict(),\n                'discriminator': discriminator.module.state_dict() if isinstance(discriminator, nn.DataParallel) else discriminator.state_dict(),\n                'optimizer_G': optimizer_G.state_dict(),\n                'optimizer_D': optimizer_D.state_dict(),\n            }\n            torch.save(checkpoint, f\"{checkpoint_dir}/checkpoint_epoch_{epoch+1}.pth\")\n\n    writer.close()\n\n# Initialize Models\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngenerator = UNetGenerator().to(device)\ndiscriminator = PatchGANDiscriminator().to(device)\n\n# Initialize Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n\n# Try to Load Latest Checkpoint\ncheckpoint_dir = \"/kaggle/working/checkpoints\"\ncheckpoint_path = f\"/kaggle/input/cvpr_gan/pytorch/default/3/checkpoint_epoch_400.pth\"\nstart_epoch = 0\n\nif os.path.exists(checkpoint_path):\n    print(f\"Loading checkpoint from {checkpoint_path}\")\n    start_epoch = load_checkpoint(checkpoint_path, generator, discriminator, device)\n    print(f\"Resuming training from epoch {start_epoch}\")\n\n# Continue Training from the Loaded Checkpoint\ntrain(generator, discriminator, train_dataloader, optimizer_G, optimizer_D,\n      device, start_epoch=start_epoch, epochs=1, checkpoint_dir=checkpoint_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:41:49.722757Z","iopub.execute_input":"2025-02-03T15:41:49.723074Z","iopub.status.idle":"2025-02-03T15:46:55.987310Z","shell.execute_reply.started":"2025-02-03T15:41:49.723044Z","shell.execute_reply":"2025-02-03T15:46:55.986361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\nimport os\nfrom torchvision import transforms\n\nclass LowLightTestDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        \"\"\"\n        Args:\n            root_dir (str): Path to the directory containing low-light images.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n        self.root_dir = root_dir\n        self.transform = transform\n\n        # Get the list of image filenames in the directory\n        self.image_paths = sorted(os.listdir(root_dir))  # assuming all images are in the root directory\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        # Load the image\n        image_path = os.path.join(self.root_dir, self.image_paths[idx])\n        image = Image.open(image_path).convert('RGB')\n\n        # Apply transformations if any\n        if self.transform:\n            image = self.transform(image)\n\n        return image\n","metadata":{"id":"JXrOtWkyGLfh","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:46:55.988514Z","iopub.execute_input":"2025-02-03T15:46:55.988866Z","iopub.status.idle":"2025-02-03T15:46:55.994698Z","shell.execute_reply.started":"2025-02-03T15:46:55.988829Z","shell.execute_reply":"2025-02-03T15:46:55.993822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torchvision import transforms\n\n# Define transformations for the test images (resize, normalize, etc.)\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),  # Resize to the expected size\n    transforms.ToTensor(),  # Convert image to tensor\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n])\n\n# Create the test dataset\ntest_dataset = LowLightTestDataset(root_dir=test_path, transform=transform)\n\n# Create the test dataloader\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"id":"OdV-awKPGTbU","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:46:55.995670Z","iopub.execute_input":"2025-02-03T15:46:55.996125Z","iopub.status.idle":"2025-02-03T15:46:56.020999Z","shell.execute_reply.started":"2025-02-03T15:46:55.996089Z","shell.execute_reply":"2025-02-03T15:46:56.020267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import matplotlib.pyplot as plt\n\n# def test(generator, dataloader, device):\n#     generator.eval()\n\n#     with torch.no_grad():\n#         low_imgs = next(iter(dataloader)).to(device)\n#         generated_high_imgs = generator(low_imgs)\n\n#         low_imgs = low_imgs.cpu().numpy()\n#         generated_high_imgs = generated_high_imgs.cpu().numpy()\n\n#         low_imgs = (low_imgs + 1) / 2\n#         generated_high_imgs = (generated_high_imgs + 1) / 2\n\n#         fig, axes = plt.subplots(len(low_imgs), 2, figsize=(8, 4 * len(low_imgs)))\n\n#         for i in range(len(low_imgs)):\n#             axes[i, 0].imshow(low_imgs[i].transpose(1, 2, 0))\n#             axes[i, 0].set_title(f\"Low {i+1}\")\n#             axes[i, 0].axis('off')\n\n#             axes[i, 1].imshow(generated_high_imgs[i].transpose(1, 2, 0))\n#             axes[i, 1].set_title(f\"Generated High {i+1}\")\n#             axes[i, 1].axis('off')\n\n#         plt.tight_layout()\n#         plt.show()\n\n# # Perform test\n# test(generator, test_dataloader, device)\n","metadata":{"id":"AQYoPV2_GOzH","outputId":"2dc94cf5-5472-4ff3-eb35-0cab19b12b57","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:46:56.021810Z","iopub.execute_input":"2025-02-03T15:46:56.022355Z","iopub.status.idle":"2025-02-03T15:47:06.021923Z","shell.execute_reply.started":"2025-02-03T15:46:56.022327Z","shell.execute_reply":"2025-02-03T15:47:06.020327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\n\ndef validate(generator, dataloader, device):\n    generator.eval()\n\n    with torch.no_grad():\n        low_imgs, high_imgs = next(iter(dataloader))\n        low_imgs, high_imgs = low_imgs.to(device), high_imgs.to(device)\n\n        generated_high_imgs = generator(low_imgs)\n\n        generated_high_imgs = generated_high_imgs.cpu().numpy()\n        high_imgs = high_imgs.cpu().numpy()\n        low_imgs = low_imgs.cpu().numpy()\n\n        generated_high_imgs = (generated_high_imgs + 1) / 2\n        high_imgs = (high_imgs + 1) / 2\n        low_imgs = (low_imgs + 1) / 2\n\n        num_images = len(low_imgs)\n        fig, axes = plt.subplots(num_images, 3, figsize=(10, 5 * num_images))\n\n        for i in range(num_images):\n            axes[i, 0].imshow(high_imgs[i].transpose(1, 2, 0))\n            axes[i, 0].set_title(f\"Original High {i+1}\")\n            axes[i, 0].axis('off')\n\n            axes[i, 1].imshow(low_imgs[i].transpose(1, 2, 0))\n            axes[i, 1].set_title(f\"Low {i+1}\")\n            axes[i, 1].axis('off')\n\n            axes[i, 2].imshow(generated_high_imgs[i].transpose(1, 2, 0))\n            axes[i, 2].set_title(f\"Generated High {i+1}\")\n            axes[i, 2].axis('off')\n\n        plt.tight_layout()\n        plt.show()\n\n# Perform validation\nvalidate(generator, val_dataloader, device)\n","metadata":{"id":"YLoZOIHVUUvN","outputId":"c360b252-f04b-4113-edf5-acf3c82b8f71","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:47:06.025547Z","iopub.execute_input":"2025-02-03T15:47:06.026158Z","iopub.status.idle":"2025-02-03T15:47:19.920980Z","shell.execute_reply.started":"2025-02-03T15:47:06.026080Z","shell.execute_reply":"2025-02-03T15:47:19.919290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_path = \"/kaggle/input/lli-dataset/LLI_dataset\"\n\n# Create datasets\neval_dataset = LowLightDataset(root_dir=eval_path, transform=transform)\neval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\nvalidate(generator, eval_dataloader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:47:19.923489Z","iopub.execute_input":"2025-02-03T15:47:19.923994Z","iopub.status.idle":"2025-02-03T15:47:33.886189Z","shell.execute_reply.started":"2025-02-03T15:47:19.923952Z","shell.execute_reply":"2025-02-03T15:47:33.885257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import shutil\n\n# # Zip the checkpoints directory\n# shutil.make_archive('/kaggle/working/checkpoints', 'zip', '/kaggle/working/checkpoints')\n\n# # Create a download link for the zip file\n# from IPython.display import FileLink\n# FileLink('/kaggle/working/checkpoints.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:47:33.887079Z","iopub.execute_input":"2025-02-03T15:47:33.887346Z","iopub.status.idle":"2025-02-03T15:47:33.892219Z","shell.execute_reply.started":"2025-02-03T15:47:33.887324Z","shell.execute_reply":"2025-02-03T15:47:33.889772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ngenerator_params = count_parameters(generator)\ndiscriminator_params = count_parameters(discriminator)\n\nprint(f\"Generator Parameters: {generator_params:,}\")\nprint(f\"Discriminator Parameters: {discriminator_params:,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:47:33.893104Z","iopub.execute_input":"2025-02-03T15:47:33.893434Z","iopub.status.idle":"2025-02-03T15:47:33.907732Z","shell.execute_reply.started":"2025-02-03T15:47:33.893399Z","shell.execute_reply":"2025-02-03T15:47:33.906994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install lpips scikit-image piq\n!pip install lpips","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:50:31.717866Z","iopub.execute_input":"2025-02-03T15:50:31.718195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport lpips\nimport torchvision.transforms as transforms\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\nfrom piq import niqe\nimport matplotlib.pyplot as plt\n\n# Load LPIPS model\nlpips_model = lpips.LPIPS(net='alex').to(device)\n\n# Transform for LPIPS & NIQE\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((256, 256)),  # Resize for NIQE compatibility\n])\n\ndef compute_metrics(generator, dataloader, device):\n    generator.eval()\n    \n    psnr_list, ssim_list, lpips_list, niqe_list = [], [], [], []\n    \n    with torch.no_grad():\n        for low_imgs, high_imgs in dataloader:\n            low_imgs, high_imgs = low_imgs.to(device), high_imgs.to(device)\n            generated_high_imgs = generator(low_imgs).cpu().numpy()\n            high_imgs = high_imgs.cpu().numpy()\n\n            # Normalize to [0,1] for SSIM and PSNR\n            generated_high_imgs = (generated_high_imgs + 1) / 2\n            high_imgs = (high_imgs + 1) / 2\n\n            for i in range(len(low_imgs)):\n                gt = np.clip(high_imgs[i].transpose(1, 2, 0), 0, 1)\n                pred = np.clip(generated_high_imgs[i].transpose(1, 2, 0), 0, 1)\n\n                # PSNR\n                psnr_value = psnr(gt, pred, data_range=1.0)\n                psnr_list.append(psnr_value)\n\n                # SSIM\n                ssim_value = ssim(gt, pred, data_range=1.0, multichannel=True)\n                ssim_list.append(ssim_value)\n\n                # LPIPS (convert to tensor)\n                gt_tensor = transform(gt).unsqueeze(0).to(device)\n                pred_tensor = transform(pred).unsqueeze(0).to(device)\n                lpips_value = lpips_model(gt_tensor, pred_tensor).item()\n                lpips_list.append(lpips_value)\n\n                # NIQE\n                niqe_value = niqe(torch.tensor(pred).permute(2, 0, 1).unsqueeze(0))\n                niqe_list.append(niqe_value.item())\n\n    print(f\"Avg PSNR: {np.mean(psnr_list):.4f}\")\n    print(f\"Avg SSIM: {np.mean(ssim_list):.4f}\")\n    print(f\"Avg LPIPS: {np.mean(lpips_list):.4f}\")\n    print(f\"Avg NIQE: {np.mean(niqe_list):.4f}\")\n\n    return {\n        \"PSNR\": np.mean(psnr_list),\n        \"SSIM\": np.mean(ssim_list),\n        \"LPIPS\": np.mean(lpips_list),\n        \"NIQE\": np.mean(niqe_list),\n    }\n\n# Run Evaluation\nmetrics = compute_metrics(generator, val_dataloader, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T15:51:23.415587Z","iopub.execute_input":"2025-02-03T15:51:23.415887Z","iopub.status.idle":"2025-02-03T15:51:23.444201Z","shell.execute_reply.started":"2025-02-03T15:51:23.415866Z","shell.execute_reply":"2025-02-03T15:51:23.442947Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-cdf89c5b7c41>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpeak_signal_noise_ratio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpsnr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructural_similarity\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mssim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpiq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mniqe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'niqe' from 'piq' (/usr/local/lib/python3.10/dist-packages/piq/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'niqe' from 'piq' (/usr/local/lib/python3.10/dist-packages/piq/__init__.py)","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}